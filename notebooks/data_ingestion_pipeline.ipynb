{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d36b500",
   "metadata": {},
   "source": [
    "# Step-by-step Ingestion of the Adver-City Dataset \n",
    "\n",
    "This notebook walks through the steps to ingest the Adver-City dataset. It does this in the follow steps:\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Configure](#1-configure)\n",
    "2. [Download from Remote Server](#2-download-from-remote-server)\n",
    "3. [Develop Manifest](#3-develop-manifest)\n",
    "4. [Build a Sampling Plan](#4-build-a-sampling-plan)\n",
    "5. [Extract based on Sampling Plan](#5-extract-based-on-sampling-plan)\n",
    "6. [Labelling and Metadata](#6-labelling-and-metadata)\n",
    "7. [Generate Train/Val/Test sets](#7-generate-trainvaltest-sets)\n",
    "8. [Conclusion](#8-conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db7da47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changed to root directory\n"
     ]
    }
   ],
   "source": [
    "# proper imports \n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "\n",
    "# define root directory\n",
    "cwd = Path.cwd()\n",
    "if cwd.name == \"notebooks\":\n",
    "    os.chdir(cwd.parent)\n",
    "    print(\"changed to root directory\")\n",
    "else:\n",
    "    print(\"already in project root\")\n",
    "\n",
    "# custom imports\n",
    "from src.ingestion import download, archive, sample, extract, label, split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30461580",
   "metadata": {},
   "source": [
    "## 1. Configure\n",
    "\n",
    "Import necessary paths and configurations from `config.json`.\n",
    "\n",
    "Note: all paths will be expressed as a Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c53f7006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected prefixes:  ['rcnj', 'ri', 'unj']\n",
      "selected weather:  ['cn', 'fn', 'hrn', 'srn']\n",
      "selected density:  ['s']\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = Path.cwd()\n",
    "CONFIG_PATH = PROJECT_ROOT / \"config\" / \"config.json\"\n",
    "\n",
    "# pull configuration files\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "# data paths\n",
    "DATA_ROOT   = PROJECT_ROOT / cfg[\"data_paths\"][\"root\"]      # root directory for all data \n",
    "RAW_DIR     = PROJECT_ROOT / cfg[\"data_paths\"][\"raw\"]       # where the raw data is stored\n",
    "INDEX_DIR   = PROJECT_ROOT / cfg[\"data_paths\"][\"index\"]     # where the data index is stored\n",
    "SAMPLED_DIR = PROJECT_ROOT / cfg[\"data_paths\"][\"sampled\"]   # where the sampled data (i.e., subset) is stored\n",
    "READY_DIR   = PROJECT_ROOT / cfg[\"data_paths\"][\"ready\"]     # where the training/val/test sets are stored\n",
    "\n",
    "# make the directories \n",
    "for p in [RAW_DIR, INDEX_DIR, SAMPLED_DIR, READY_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# archive info\n",
    "BASE_URL    = cfg[\"ingestion\"][\"url\"]                       # remote location of data archive\n",
    "ARCHIVE_EXT = cfg[\"ingestion\"][\"archive_extension\"]         # archive file extension \n",
    "MANIFEST_MODE = cfg[\"ingestion\"][\"manifest_mode\"]           # mode for manifest generation (simple/verbose)\n",
    "\n",
    "# sampling \n",
    "MAX_GB      = float(cfg[\"ingestion\"][\"max_size_GB\"])        # maximum file size\n",
    "MAX_IMGS    = int(cfg[\"ingestion\"][\"images_per_archive\"])   # maximum number of images to pull\n",
    "STRIDE      = int(cfg[\"ingestion\"][\"frame_stride\"])          # gaps between frames when sampling \n",
    "SEED = int(cfg[\"reproducibility\"][\"seed\"])\n",
    "random.seed(SEED)\n",
    "CLEANUP_RAW = cfg[\"sampling\"].get(\"cleanup_raw_after_extract\", False)\n",
    "\n",
    "# sampling plan config\n",
    "PLAN_FILENAME = cfg[\"sampling\"][\"plan_filename\"]            # filename for sample plan\n",
    "PLAN_OVERWRITE = cfg[\"sampling\"][\"overwrite\"]               # whether to overwrite existing plan\n",
    "LABELS_FILENAME = cfg[\"sampling\"][\"labels_filename\"]        # filename for labels CSV\n",
    "\n",
    "# image info\n",
    "CAMERA     = cfg[\"ingestion\"].get(\"camera\", None)           # safer\n",
    "IMG_TYPE   = cfg[\"ingestion\"][\"image_type\"]                 # rgb\n",
    "IMG_EXT    = cfg[\"ingestion\"][\"image_extension\"]            # file extension of images \n",
    "\n",
    "# labelling info\n",
    "labels_cfg = cfg[\"labels\"]\n",
    "\n",
    "# training/splits info\n",
    "SPLITS_TRAIN = cfg[\"splits\"][\"train\"]\n",
    "SPLITS_VAL   = cfg[\"splits\"][\"val\"]\n",
    "SPLITS_TEST  = cfg[\"splits\"][\"test\"]\n",
    "SPLITS_OVERWRITE =cfg[\"splits\"][\"overwrite\"]\n",
    "CLEANUP_SAMPLED = cfg[\"splits\"].get(\"cleanup_sampled_after_split\", False)\n",
    "\n",
    "# the valid file types\n",
    "VALID_PREFIX  = set(labels_cfg[\"valid_prefix\"])\n",
    "VALID_WEATHER = set(labels_cfg[\"valid_weather\"])\n",
    "VALID_DENSITY = set(labels_cfg[\"valid_density\"])\n",
    "\n",
    "# the files I want to download\n",
    "CHOOSE_PREFIX  = labels_cfg.get(\"choose_prefix\", labels_cfg[\"valid_prefix\"])\n",
    "CHOOSE_WEATHER = labels_cfg.get(\"choose_weather\", labels_cfg[\"valid_weather\"])\n",
    "CHOOSE_DENSITY = labels_cfg.get(\"choose_density\", labels_cfg[\"valid_density\"])\n",
    "\n",
    "# decoders (two separate label spaces)\n",
    "DECODE_TIME = labels_cfg[\"weather_decode_time\"]             # maps files to day/night\n",
    "DECODE_VIS  = labels_cfg[\"weather_decode_visibility\"]       # maps files to visibility conditions\n",
    "\n",
    "print('selected prefixes: ', CHOOSE_PREFIX)\n",
    "print('selected weather: ', CHOOSE_WEATHER)\n",
    "print('selected density: ', CHOOSE_DENSITY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6863d0e",
   "metadata": {},
   "source": [
    "## 2. Download from Remote Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32108026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build the following filenames: \n",
      " ['rcnj_cn_s.7z', 'rcnj_fn_s.7z', 'rcnj_hrn_s.7z', 'rcnj_srn_s.7z', 'ri_cn_s.7z', 'ri_fn_s.7z', 'ri_hrn_s.7z', 'ri_srn_s.7z', 'unj_cn_s.7z', 'unj_fn_s.7z', 'unj_hrn_s.7z', 'unj_srn_s.7z']\n"
     ]
    }
   ],
   "source": [
    "# note: enforce lists for choices\n",
    "\n",
    "filenames = download.build_filenames(CHOOSE_PREFIX, CHOOSE_WEATHER, CHOOSE_DENSITY, \n",
    "                               VALID_PREFIX, VALID_WEATHER, VALID_DENSITY, \n",
    "                               ARCHIVE_EXT)\n",
    "print('build the following filenames: \\n', filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dc3c7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] rcnj_cn_s.7z already present in raw.\n",
      "[SKIP] rcnj_fn_s.7z already present in raw.\n",
      "[SKIP] rcnj_hrn_s.7z already present in raw.\n",
      "[SKIP] rcnj_srn_s.7z already present in raw.\n",
      "[SKIP] ri_cn_s.7z already present in raw.\n",
      "[SKIP] ri_fn_s.7z already present in raw.\n",
      "[SKIP] ri_hrn_s.7z already present in raw.\n",
      "[SKIP] ri_srn_s.7z already present in raw.\n",
      "[SKIP] unj_cn_s.7z already present in raw.\n",
      "[SKIP] unj_fn_s.7z already present in raw.\n",
      "[SKIP] unj_hrn_s.7z already present in raw.\n",
      "[SKIP] unj_srn_s.7z already present in raw.\n",
      "Downloaded  12 files.\n",
      "--> rcnj_cn_s.7z\n",
      "--> rcnj_fn_s.7z\n",
      "--> rcnj_hrn_s.7z\n",
      "--> rcnj_srn_s.7z\n",
      "--> ri_cn_s.7z\n",
      "--> ri_fn_s.7z\n",
      "--> ri_hrn_s.7z\n",
      "--> ri_srn_s.7z\n",
      "--> unj_cn_s.7z\n",
      "--> unj_fn_s.7z\n",
      "--> unj_hrn_s.7z\n",
      "--> unj_srn_s.7z\n"
     ]
    }
   ],
   "source": [
    "download_raw = download.download_files(\n",
    "    base_url = BASE_URL, \n",
    "    destinations_dir = RAW_DIR, \n",
    "    filenames = filenames, \n",
    "    timeout = 60, \n",
    "    max_size_GB = MAX_GB, \n",
    "    overwrite = False\n",
    ")\n",
    "\n",
    "print('Downloaded ', len(download_raw), 'files.')\n",
    "for file in download_raw:\n",
    "    print('-->', file.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993cc260",
   "metadata": {},
   "source": [
    "## 3. Develop Manifest\n",
    "\n",
    "Develop a manifest of the files downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2d5e489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 downloaded archives\n",
      "\n",
      "[SKIP] Manifest already exists:\n",
      "  rcnj_cn_s_manifest.json\n",
      "  rcnj_cn_s.7z: 10032 lines\n",
      "[SKIP] Manifest already exists:\n",
      "  rcnj_fn_s_manifest.json\n",
      "  rcnj_fn_s.7z: 10017 lines\n",
      "[SKIP] Manifest already exists:\n",
      "  rcnj_hrn_s_manifest.json\n",
      "  rcnj_hrn_s.7z: 10025 lines\n",
      "[SKIP] Manifest already exists:\n",
      "  rcnj_srn_s_manifest.json\n",
      "  rcnj_srn_s.7z: 10032 lines\n",
      "[SKIP] Manifest already exists:\n",
      "  ri_cn_s_manifest.json\n",
      "  ri_cn_s.7z: 7781 lines\n",
      "[SKIP] Manifest already exists:\n",
      "  ri_fn_s_manifest.json\n",
      "  ri_fn_s.7z: 7755 lines\n",
      "[SKIP] Manifest already exists:\n",
      "  ri_hrn_s_manifest.json\n",
      "  ri_hrn_s.7z: 7779 lines\n",
      "[SKIP] Manifest already exists:\n",
      "  ri_srn_s_manifest.json\n",
      "  ri_srn_s.7z: 7739 lines\n",
      "[SKIP] Manifest already exists:\n",
      "  unj_cn_s_manifest.json\n",
      "  unj_cn_s.7z: 6894 lines\n",
      "[SKIP] Manifest already exists:\n",
      "  unj_fn_s_manifest.json\n",
      "  unj_fn_s.7z: 6899 lines\n",
      "[SKIP] Manifest already exists:\n",
      "  unj_hrn_s_manifest.json\n",
      "  unj_hrn_s.7z: 6899 lines\n",
      "[SKIP] Manifest already exists:\n",
      "  unj_srn_s_manifest.json\n",
      "  unj_srn_s.7z: 6894 lines\n",
      "\n",
      "Total manifests: 12\n"
     ]
    }
   ],
   "source": [
    "# index the downloaded archives (without extracting)\n",
    "archives = sorted(RAW_DIR.glob(f\"*{ARCHIVE_EXT}\"))\n",
    "print(f\"Found {len(archives)} downloaded archives\\n\")\n",
    "\n",
    "# load or create manifests\n",
    "manifests = {}\n",
    "for archive_file in archives:\n",
    "    manifest = archive.build_manifest(archive_file, INDEX_DIR, mode=MANIFEST_MODE)\n",
    "    manifests[archive_file.name] = manifest\n",
    "    print(f\"  {archive_file.name}: {len(manifest)} lines\")\n",
    "\n",
    "print(f\"\\nTotal manifests: {len(manifests)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e037096",
   "metadata": {},
   "source": [
    "## 4. Build a Sampling Plan\n",
    "\n",
    "Build a sampling plan for the dataset, so we only have to extract a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed0faab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rcnj_cn_s.7z:\n",
      "  Candidates: 760\n",
      "  Sampled: 760\n",
      "\n",
      "rcnj_fn_s.7z:\n",
      "  Candidates: 760\n",
      "  Sampled: 760\n",
      "\n",
      "rcnj_hrn_s.7z:\n",
      "  Candidates: 760\n",
      "  Sampled: 760\n",
      "\n",
      "rcnj_srn_s.7z:\n",
      "  Candidates: 760\n",
      "  Sampled: 760\n",
      "\n",
      "ri_cn_s.7z:\n",
      "  Candidates: 592\n",
      "  Sampled: 592\n",
      "\n",
      "ri_fn_s.7z:\n",
      "  Candidates: 592\n",
      "  Sampled: 592\n",
      "\n",
      "ri_hrn_s.7z:\n",
      "  Candidates: 592\n",
      "  Sampled: 592\n",
      "\n",
      "ri_srn_s.7z:\n",
      "  Candidates: 592\n",
      "  Sampled: 592\n",
      "\n",
      "unj_cn_s.7z:\n",
      "  Candidates: 520\n",
      "  Sampled: 520\n",
      "\n",
      "unj_fn_s.7z:\n",
      "  Candidates: 520\n",
      "  Sampled: 520\n",
      "\n",
      "unj_hrn_s.7z:\n",
      "  Candidates: 520\n",
      "  Sampled: 520\n",
      "\n",
      "unj_srn_s.7z:\n",
      "  Candidates: 520\n",
      "  Sampled: 520\n",
      "\n",
      "\n",
      "Total images to extract: 7488\n",
      "[SKIP] Sample plan already exists at sample_plan.json. Using existing plan.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/tjards/Library/CloudStorage/Dropbox/adjunctQueens/code/pytorch_project_advercity/data/index/sample_plan.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_plan = sample.build_sample_plan(manifests, \n",
    "                      CAMERA=CAMERA, \n",
    "                      IMG_EXT=IMG_EXT, \n",
    "                      STRIDE=STRIDE, \n",
    "                      MAX_IMGS=MAX_IMGS, \n",
    "                      SEED=SEED)\n",
    "\n",
    "print(f\"\\nTotal images to extract: {sum(len(v) for v in sampling_plan.values())}\")\n",
    "\n",
    "sample.save_sample_plan(sampling_plan, \n",
    "                        INDEX_DIR / PLAN_FILENAME,\n",
    "                        overwrite=PLAN_OVERWRITE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0845ac39",
   "metadata": {},
   "source": [
    "## 5. Extract based on Sampling Plan\n",
    "\n",
    "- cross-references the SAMPLED_DIR contents with the sampling plan and only accesses the archive if files are missing (saving time)\n",
    "- `cleanup_raw`: we may choose to cleanup (delete) raw files after we have finished raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be37512b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "rcnj_cn_s.7z:\n",
      "  Files to extract: 760\n",
      "all files exist in sampled\n",
      " [SKIP] All files already extracted for rcnj_cn_s.7z\n",
      "\n",
      "rcnj_fn_s.7z:\n",
      "  Files to extract: 760\n",
      "all files exist in sampled\n",
      " [SKIP] All files already extracted for rcnj_fn_s.7z\n",
      "\n",
      "rcnj_hrn_s.7z:\n",
      "  Files to extract: 760\n",
      "all files exist in sampled\n",
      " [SKIP] All files already extracted for rcnj_hrn_s.7z\n",
      "\n",
      "rcnj_srn_s.7z:\n",
      "  Files to extract: 760\n",
      "all files exist in sampled\n",
      " [SKIP] All files already extracted for rcnj_srn_s.7z\n",
      "\n",
      "ri_cn_s.7z:\n",
      "  Files to extract: 592\n",
      "all files exist in sampled\n",
      " [SKIP] All files already extracted for ri_cn_s.7z\n",
      "\n",
      "ri_fn_s.7z:\n",
      "  Files to extract: 592\n",
      "all files exist in sampled\n",
      " [SKIP] All files already extracted for ri_fn_s.7z\n",
      "\n",
      "ri_hrn_s.7z:\n",
      "  Files to extract: 592\n",
      "all files exist in sampled\n",
      " [SKIP] All files already extracted for ri_hrn_s.7z\n",
      "\n",
      "ri_srn_s.7z:\n",
      "  Files to extract: 592\n",
      "all files exist in sampled\n",
      " [SKIP] All files already extracted for ri_srn_s.7z\n",
      "\n",
      "unj_cn_s.7z:\n",
      "  Files to extract: 520\n",
      "all files exist in sampled\n",
      " [SKIP] All files already extracted for unj_cn_s.7z\n",
      "\n",
      "unj_fn_s.7z:\n",
      "  Files to extract: 520\n",
      "all files exist in sampled\n",
      " [SKIP] All files already extracted for unj_fn_s.7z\n",
      "\n",
      "unj_hrn_s.7z:\n",
      "  Files to extract: 520\n",
      "all files exist in sampled\n",
      " [SKIP] All files already extracted for unj_hrn_s.7z\n",
      "\n",
      "unj_srn_s.7z:\n",
      "  Files to extract: 520\n",
      "all files exist in sampled\n",
      " [SKIP] All files already extracted for unj_srn_s.7z\n",
      "\n",
      "==================================================\n",
      "Extraction Summary:\n",
      "  Total archives processed: 12\n",
      "  Total files extracted: 0\n",
      "  Total files skipped: 7488\n",
      "  Total errors: 0\n",
      "==================================================\n",
      "\n",
      "Extraction complete. Check SAMPLED_DIR for extracted files:\n",
      "  sampled\n"
     ]
    }
   ],
   "source": [
    "# define paths\n",
    "sample_plan_file = INDEX_DIR / PLAN_FILENAME\n",
    "\n",
    "# extract files based on sampling plan\n",
    "extraction_results = extract.extract_from_sample_plan(\n",
    "    sample_plan_file=sample_plan_file,\n",
    "    raw_dir=RAW_DIR,\n",
    "    sampled_dir=SAMPLED_DIR,\n",
    "    overwrite=PLAN_OVERWRITE,\n",
    "    cleanup_raw=CLEANUP_RAW\n",
    ")\n",
    "\n",
    "print(f\"\\nExtraction complete. Check SAMPLED_DIR for extracted files:\")\n",
    "print(f\"  {SAMPLED_DIR.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f94e272",
   "metadata": {},
   "source": [
    "## 6. Labelling and Metadata\n",
    "\n",
    "Build and save a dataframe with labels and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d349ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] .DS_Store is not a directory\n",
      "\n",
      "Labeled 7488 images\n",
      "[SAVE] Labels saved to labels.csv\n",
      " Total images: 7488\n",
      " Columns: ['archive_name', 'image_path', 'prefix', 'weather', 'density', 'time', 'visibility', 'agent_id', 'frame_id', 'camera_id']\n",
      " head:\n",
      "  archive_name                       image_path prefix weather density   time  \\\n",
      "0    rcnj_cn_s  rcnj_cn_s/-1/000242_camera2.png   rcnj      cn       s  night   \n",
      "1    rcnj_cn_s  rcnj_cn_s/-1/000188_camera3.png   rcnj      cn       s  night   \n",
      "2    rcnj_cn_s  rcnj_cn_s/-1/000232_camera2.png   rcnj      cn       s  night   \n",
      "3    rcnj_cn_s  rcnj_cn_s/-1/000147_camera2.png   rcnj      cn       s  night   \n",
      "4    rcnj_cn_s  rcnj_cn_s/-1/000137_camera2.png   rcnj      cn       s  night   \n",
      "\n",
      "  visibility agent_id frame_id camera_id  \n",
      "0      clear       -1   000242   camera2  \n",
      "1      clear       -1   000188   camera3  \n",
      "2      clear       -1   000232   camera2  \n",
      "3      clear       -1   000147   camera2  \n",
      "4      clear       -1   000137   camera2  \n"
     ]
    }
   ],
   "source": [
    "# build labels dataframe\n",
    "labels_df = label.build_labels_df(\n",
    "    sampled_dir=SAMPLED_DIR,\n",
    "    decode_time=DECODE_TIME,\n",
    "    decode_vis=DECODE_VIS,\n",
    "    archive_ext=ARCHIVE_EXT,\n",
    "    img_ext=IMG_EXT\n",
    ")\n",
    "\n",
    "print(f\"\\nLabeled {len(labels_df)} images\")\n",
    "\n",
    "# save to CSV\n",
    "labels_csv = label.save_labels(\n",
    "    df = labels_df, \n",
    "    output_path = INDEX_DIR / LABELS_FILENAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840083da",
   "metadata": {},
   "source": [
    "## 7. Generate Train/Val/Test sets \n",
    "\n",
    "- `cleanup_sampled`: we may choose to cleanup (delete) sampled files after we have finished processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c91601cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] Loaded 7488 labels from: labels.csv\n",
      "Train: 5241 images\n",
      "Val: 1123 images\n",
      "Test: 1124 images\n"
     ]
    }
   ],
   "source": [
    "splits = split.split_labels(\n",
    "    labels_path=INDEX_DIR / LABELS_FILENAME,\n",
    "    train_ratio=SPLITS_TRAIN,\n",
    "    val_ratio=SPLITS_VAL,\n",
    "    test_ratio=SPLITS_TEST,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# Check results\n",
    "print(f\"Train: {len(splits['train'])} images\")\n",
    "print(f\"Val: {len(splits['val'])} images\")\n",
    "print(f\"Test: {len(splits['test'])} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e36ce7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SKIP] Split 'train' already exists. Use overwrite=True to rebuild.\n",
      "\n",
      "[SKIP] Split 'val' already exists. Use overwrite=True to rebuild.\n",
      "\n",
      "[SKIP] Split 'test' already exists. Use overwrite=True to rebuild.\n",
      "\n",
      "Train/Val/Test split complete!\n",
      "  Train: train/\n",
      "  Val: val/\n",
      "  Test: test/\n"
     ]
    }
   ],
   "source": [
    "# build files into train/val/test directories\n",
    "split.build_splits(\n",
    "    splits=splits,\n",
    "    sampled_dir=SAMPLED_DIR,\n",
    "    ready_dir=READY_DIR, \n",
    "    cleanup_sampled=CLEANUP_SAMPLED \n",
    ")\n",
    "\n",
    "print(f\"\\nTrain/Val/Test split complete!\")\n",
    "print(f\"  Train: {(READY_DIR / 'train').name}/\")\n",
    "print(f\"  Val: {(READY_DIR / 'val').name}/\")\n",
    "print(f\"  Test: {(READY_DIR / 'test').name}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f52d81",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "And now we should have the nicely organized datasets under `/data/ready/...`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
